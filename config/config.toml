[models]
# DEPRECATED: No longer used by RecipeLLMClassifier. All settings moved to [llm].
# Migrate to [llm] section below. Remove after migration.
# Legacy model configuration - these settings are no longer used
zero_shot_model = "qwen3:14b"
default_llm_model = "qwen/qwen3-vl-8b"
llm_provider = "openai"
llm_api_base = "http://localhost:1234/v1"
default_timeout = 30
request_timeout = 60

[llm]
# LLM Provider Configuration
# ===========================
# This section replaces the legacy OLLAMA_IP and OLLAMA_PORT environment variables.
# All LLM provider settings (including Ollama) are now configured here in config.toml.
#
# API Key Setup
# -------------
# Set via environment variables:
#   export OPENAI_API_KEY='sk-...'              # For OpenAI
#   export OPENAI_API_KEY='dummy'               # For LM Studio (any value works)
#   export ANTHROPIC_API_KEY='sk-ant-...'       # For Anthropic
# Ollama: No API key needed
#
# Switching Providers in Code
# ----------------------------
# Usage:
#   RecipeLLMClassifier()                        # Uses default_provider below
#   RecipeLLMClassifier(provider_name='openai')  # Uses OpenAI
#   RecipeLLMClassifier(provider_name='ollama')  # Uses Ollama
#
# Fallback Behavior
# -----------------
# If enable_fallback=true, the classifier tries providers in order on transient errors
#
# Migration from .env:
#   - Old: OLLAMA_IP and OLLAMA_PORT in config/.env
#   - New: Configure Ollama provider below with full api_base URL

# Default provider to use (must match one of the provider names below)
default_provider = "lm_studio"

# Enable automatic fallback to other providers on failure
enable_fallback = true

# Provider configurations
# API keys are loaded from environment variables (see above)

[[llm.providers]]
name = "lm_studio"
model = "qwen/qwen3-vl-8b"  # Model name as shown in LM Studio
api_base = "http://localhost:1234/v1"  # LM Studio on default port
timeout = 60
max_retries = 3
temperature = 0.7
# API key loaded from OPENAI_API_KEY env var (LM Studio uses OpenAI-compatible format)

# [[llm.providers]]
# name = "ollama"
# model = "qwen3:14b"
# api_base = "http://localhost:11434"  # Local endpoint; ensure Ollama is running with model loaded
# timeout = 30
# max_retries = 3
# temperature = 0.7

# [[llm.providers]]
# name = "openai"
# model = "gpt-4o-mini"  # Cost-effective model; upgrade to gpt-4o for better accuracy
# timeout = 30
# max_retries = 3
# temperature = 0.7
# max_tokens = 1000
# # API key loaded from OPENAI_API_KEY env var

# [[llm.providers]]
# name = "anthropic"
# model = "claude-3-5-sonnet-20241022"  # High-quality model for production
# timeout = 30
# max_retries = 3
# temperature = 0.7
# max_tokens = 1000
# API key loaded from ANTHROPIC_API_KEY env var

[classification]
# Recipe classification labels
recipe_positive_label = "this text contains a recipe with ingredients and/or cooking instructions"
recipe_negative_label = "this text does not contain any recipe or cooking instructions"

# Classification confidence thresholds
min_confidence_threshold = 0.6
max_confidence_threshold = 1.0

# Multi-class classification class definitions
# These are default classes available to MultiClassLLMClassifier
# You can override or extend these at runtime via the classifier constructor or API request

[[classification.classes]]
name = "recipe"
description = """A text containing cooking instructions, ingredients, recipe details, or food preparation steps. 
Examples: 'Boil pasta for 10 minutes', 'Mix flour and eggs', 'Preheat oven to 350Â°F'"""

[[classification.classes]]
name = "python_package"
description = """A text about Python packages, libraries, pip installations, or Python development tools. 
Examples: 'Install FastAPI with pip', 'This library provides async support', 'Check out this new Django package'"""

[[classification.classes]]
name = "movie_review"
description = """A text reviewing, discussing, or mentioning movies, films, cinema, or TV shows. 
Examples: 'Just watched an amazing thriller', 'This movie has great cinematography', 'The acting was superb'"""

[[classification.classes]]
name = "tech_news"
description = """A text about technology news, product launches, tech industry updates, or software releases. 
Examples: 'Apple announced new iPhone', 'OpenAI releases GPT-5', 'Microsoft acquires startup'"""

[[classification.classes]]
name = "other"
description = """Any text that doesn't fit into the above categories. General content, personal updates, or miscellaneous topics."""

[prompts]
# LLM prompts for various classification tasks
recipe_analysis_prompt = """Analyze if the following content is a recipe and extract key details.

Content: {content}

{format_instructions}

If it's not a recipe, set is_recipe to false and leave other fields as null."""

[database]
# Database configuration
default_db_path = "data/social_media.db"

[api]
# FastAPI server configuration
host = "0.0.0.0"
port = 8000
reload = true  # Enable auto-reload in development
workers = 1    # Number of worker processes (increase for production)
log_level = "info"  # Logging level (debug, info, warning, error)
environment = "development"  # Environment mode (development, production)

# Logging configuration
log_request_body = false  # Log request bodies (enable for debugging, disable for production)
log_response_body = false  # Log response bodies (enable for debugging, disable for production)
log_format = "text"  # Log format: "text" or "json" (use "json" for structured logging in production)

# API rate limiting and request management (legacy - for parsers)
max_requests_per_session = 100
request_delay_min = 1.0
request_delay_max = 3.0
max_retries = 3

[api.auth]
# JWT authentication configuration
enabled = false  # Disable auth for development, enable for production
algorithm = "HS256"
token_expiry = 3600  # Token expiration in seconds (1 hour)
# secret_key should be set via JWT_SECRET_KEY environment variable

[api.cors]
# CORS configuration for frontend
allowed_origins = ["http://localhost:3000", "http://localhost:3001"]  # NextJS dev servers
allow_credentials = true
allowed_methods = ["*"]
allowed_headers = ["*"]

[api.rate_limiting]
# Rate limiting configuration
# Enable in production to prevent abuse. Disable for development if needed.
enabled = true
requests_per_minute = 60
burst_size = 10

[api.jobs]
# Job management configuration
max_job_age_hours = 24  # Clean up jobs older than 24 hours
cleanup_interval_minutes = 60  # Run cleanup every hour

[api.websocket]
# WebSocket configuration
heartbeat_interval = 30  # Send heartbeat every 30 seconds to keep connection alive
max_connections_per_job = 10  # Maximum WebSocket connections per job

[api.cache]
# Redis caching configuration (optional)
# Enable for production with high traffic to improve response times
# When Redis is unavailable, the system automatically falls back to in-memory caching
enabled = false  # Enable for production, disable for development
redis_url = "redis://localhost:6379/0"  # Redis connection URL (can be overridden by REDIS_URL env var)
default_ttl = 300  # Default cache TTL in seconds (5 minutes)
search_ttl = 600  # Cache TTL for search results (10 minutes)
max_memory_mb = 100  # Max memory for in-memory fallback cache (MB)

[paths]
# Default directory paths
cache_dir = "data/cache"
downloads_dir = "data/downloads"
telegram_downloads_dir = "data/downloads/telegram"
instagram_downloads_dir = "data/downloads/instagram"
models_dir = "models"
reports_dir = "reports"

[telegram]
# Telegram-specific configuration
connection_retries = 3
retry_delay = 1
auto_reconnect = true
request_retries = 3
media_timeout_image = 30
media_timeout_document = 60
connection_delay_min = 2.0
connection_delay_max = 4.0
request_delay_base = 2.0
request_delay_increment = 0.5
extra_delay_every_10_min = 10.0
extra_delay_every_10_max = 15.0
long_delay_every_50_min = 20.0
long_delay_every_50_max = 30.0
save_delay_min = 0.1
save_delay_max = 0.3

[instagram]
# Instagram-specific configuration
default_min_delay = 5.0
default_max_delay = 30.0
retry_multiplier = 5
retry_min_wait = 5
retry_max_wait = 60
login_delay_min = 1.0
login_delay_max = 2.0
request_delay_min = 2.0
request_delay_max = 4.0
general_delay_min = 1.0
general_delay_max = 2.0
