[models]
# DEPRECATED: This section is kept for backward compatibility. Use [llm] section instead.
# Legacy model configuration - these settings are no longer used by RecipeClassifier
zero_shot_model = "qwen3:14b"
default_llm_model = "qwen/qwen3-vl-8b"
llm_provider = "openai"
llm_api_base = "http://localhost:1234/v1"
default_timeout = 30
request_timeout = 60

[llm]
# LLM Provider Configuration
# ===========================
# This section replaces the legacy OLLAMA_IP and OLLAMA_PORT environment variables.
# All LLM provider settings (including Ollama) are now configured here in config.toml.
#
# To override settings, you can use environment variables or modify this file:
#   - Set OPENAI_API_KEY for OpenAI, LM Studio, or other OpenAI-compatible endpoints
#   - Set ANTHROPIC_API_KEY for Anthropic Claude models
#   - Ollama endpoints are configured via api_base below (no API key needed)
#
# Migration from .env:
#   - Old: OLLAMA_IP and OLLAMA_PORT in config/.env
#   - New: Configure Ollama provider below with full api_base URL

# Default provider to use (must match one of the provider names below)
default_provider = "lm_studio"

# Enable automatic fallback to other providers on failure
enable_fallback = true

# Provider configurations (first in list is primary, rest are fallbacks)
# API keys are loaded from environment variables:
#   - OpenAI: OPENAI_API_KEY
#   - Anthropic: ANTHROPIC_API_KEY
#   - LM Studio: OPENAI_API_KEY (uses OpenAI-compatible format)
#   - Ollama: No API key needed for local deployment

[[llm.providers]]
name = "lm_studio"
model = "qwen/qwen3-vl-8b"  # Model name as shown in LM Studio
api_base = "http://localhost:1234/v1"
timeout = 60
max_retries = 3
temperature = 0.7
# API key loaded from OPENAI_API_KEY env var (LM Studio uses OpenAI-compatible format)

[[llm.providers]]
name = "ollama"
model = "qwen3:14b"
api_base = "http://localhost:11434"  # Default Ollama endpoint
timeout = 30
max_retries = 3
temperature = 0.7

[[llm.providers]]
name = "openai"
model = "gpt-4o-mini"  # Cost-effective model for production
timeout = 30
max_retries = 3
temperature = 0.7
max_tokens = 1000
# API key loaded from OPENAI_API_KEY env var

[[llm.providers]]
name = "anthropic"
model = "claude-3-5-sonnet-20241022"
timeout = 30
max_retries = 3
temperature = 0.7
max_tokens = 1000
# API key loaded from ANTHROPIC_API_KEY env var

[classification]
# Recipe classification labels
recipe_positive_label = "this text contains a recipe with ingredients and/or cooking instructions"
recipe_negative_label = "this text does not contain any recipe or cooking instructions"

# Classification confidence thresholds
min_confidence_threshold = 0.6
max_confidence_threshold = 1.0

[prompts]
# LLM prompts for various classification tasks
recipe_analysis_prompt = """Analyze if the following content is a recipe and extract key details.

Content: {content}

{format_instructions}

If it's not a recipe, set is_recipe to false and leave other fields as null."""

[database]
# Database configuration
default_db_path = "data/social_media.db"
analysis_db_path = "data/analysis.db"

[api]
# API rate limiting and request management
max_requests_per_session = 100
request_delay_min = 1.0
request_delay_max = 3.0
max_retries = 3

[paths]
# Default directory paths
cache_dir = "data/cache"
downloads_dir = "data/downloads"
telegram_downloads_dir = "data/downloads/telegram"
instagram_downloads_dir = "data/downloads/instagram"
models_dir = "models"
reports_dir = "reports"

[telegram]
# Telegram-specific configuration
connection_retries = 3
retry_delay = 1
auto_reconnect = true
request_retries = 3
media_timeout_image = 30
media_timeout_document = 60
connection_delay_min = 2.0
connection_delay_max = 4.0
request_delay_base = 2.0
request_delay_increment = 0.5
extra_delay_every_10_min = 10.0
extra_delay_every_10_max = 15.0
long_delay_every_50_min = 20.0
long_delay_every_50_max = 30.0
save_delay_min = 0.1
save_delay_max = 0.3

[instagram]
# Instagram-specific configuration
default_min_delay = 5.0
default_max_delay = 30.0
retry_multiplier = 5
retry_min_wait = 5
retry_max_wait = 60
login_delay_min = 1.0
login_delay_max = 2.0
request_delay_min = 2.0
request_delay_max = 4.0
general_delay_min = 1.0
general_delay_max = 2.0
