[models]
# DEPRECATED: No longer used by RecipeLLMClassifier. All settings moved to [llm].
# Migrate to [llm] section below. Remove after migration.
# Legacy model configuration - these settings are no longer used
zero_shot_model = "qwen3:14b"
default_llm_model = "qwen/qwen3-vl-8b"
llm_provider = "openai"
llm_api_base = "http://localhost:1234/v1"
default_timeout = 30
request_timeout = 60

[llm]
# LLM Provider Configuration
# ===========================
# This section replaces the legacy OLLAMA_IP and OLLAMA_PORT environment variables.
# All LLM provider settings (including Ollama) are now configured here in config.toml.
#
# API Key Setup
# -------------
# Set via environment variables:
#   export OPENAI_API_KEY='sk-...'              # For OpenAI
#   export OPENAI_API_KEY='dummy'               # For LM Studio (any value works)
#   export ANTHROPIC_API_KEY='sk-ant-...'       # For Anthropic
# Ollama: No API key needed
#
# Switching Providers in Code
# ----------------------------
# Usage:
#   RecipeLLMClassifier()                        # Uses default_provider below
#   RecipeLLMClassifier(provider_name='openai')  # Uses OpenAI
#   RecipeLLMClassifier(provider_name='ollama')  # Uses Ollama
#
# Fallback Behavior
# -----------------
# If enable_fallback=true, the classifier tries providers in order on transient errors
#
# Migration from .env:
#   - Old: OLLAMA_IP and OLLAMA_PORT in config/.env
#   - New: Configure Ollama provider below with full api_base URL

# Default provider to use (must match one of the provider names below)
default_provider = "lm_studio"

# Enable automatic fallback to other providers on failure
enable_fallback = true

# Provider configurations
# API keys are loaded from environment variables (see above)

[[llm.providers]]
name = "lm_studio"
model = "qwen/qwen3-vl-8b"  # Model name as shown in LM Studio
api_base = "http://localhost:1234/v1"  # LM Studio on default port
timeout = 60
max_retries = 3
temperature = 0.7
# API key loaded from OPENAI_API_KEY env var (LM Studio uses OpenAI-compatible format)

[[llm.providers]]
name = "ollama"
model = "qwen3:14b"
api_base = "http://localhost:11434"  # Local endpoint; ensure Ollama is running with model loaded
timeout = 30
max_retries = 3
temperature = 0.7

[[llm.providers]]
name = "openai"
model = "gpt-4o-mini"  # Cost-effective model; upgrade to gpt-4o for better accuracy
timeout = 30
max_retries = 3
temperature = 0.7
max_tokens = 1000
# API key loaded from OPENAI_API_KEY env var

[[llm.providers]]
name = "anthropic"
model = "claude-3-5-sonnet-20241022"  # High-quality model for production
timeout = 30
max_retries = 3
temperature = 0.7
max_tokens = 1000
# API key loaded from ANTHROPIC_API_KEY env var

[classification]
# Recipe classification labels
recipe_positive_label = "this text contains a recipe with ingredients and/or cooking instructions"
recipe_negative_label = "this text does not contain any recipe or cooking instructions"

# Classification confidence thresholds
min_confidence_threshold = 0.6
max_confidence_threshold = 1.0

[prompts]
# LLM prompts for various classification tasks
recipe_analysis_prompt = """Analyze if the following content is a recipe and extract key details.

Content: {content}

{format_instructions}

If it's not a recipe, set is_recipe to false and leave other fields as null."""

[database]
# Database configuration
default_db_path = "data/social_media.db"
analysis_db_path = "data/analysis.db"

[api]
# FastAPI server configuration
host = "0.0.0.0"
port = 8000
reload = true  # Enable auto-reload in development
workers = 1    # Number of worker processes (increase for production)
log_level = "info"  # Logging level (debug, info, warning, error)
environment = "development"  # Environment mode (development, production)

# API rate limiting and request management (legacy - for parsers)
max_requests_per_session = 100
request_delay_min = 1.0
request_delay_max = 3.0
max_retries = 3

[api.auth]
# JWT authentication configuration
enabled = false  # Disable auth for development, enable for production
algorithm = "HS256"
token_expiry = 3600  # Token expiration in seconds (1 hour)
# secret_key should be set via JWT_SECRET_KEY environment variable

[api.cors]
# CORS configuration for frontend
allowed_origins = ["http://localhost:3000", "http://localhost:3001"]  # NextJS dev servers
allow_credentials = true
allowed_methods = ["*"]
allowed_headers = ["*"]

[api.rate_limiting]
# Rate limiting configuration (to be implemented in subsequent phase)
enabled = false
requests_per_minute = 60
burst_size = 10

[paths]
# Default directory paths
cache_dir = "data/cache"
downloads_dir = "data/downloads"
telegram_downloads_dir = "data/downloads/telegram"
instagram_downloads_dir = "data/downloads/instagram"
models_dir = "models"
reports_dir = "reports"

[telegram]
# Telegram-specific configuration
connection_retries = 3
retry_delay = 1
auto_reconnect = true
request_retries = 3
media_timeout_image = 30
media_timeout_document = 60
connection_delay_min = 2.0
connection_delay_max = 4.0
request_delay_base = 2.0
request_delay_increment = 0.5
extra_delay_every_10_min = 10.0
extra_delay_every_10_max = 15.0
long_delay_every_50_min = 20.0
long_delay_every_50_max = 30.0
save_delay_min = 0.1
save_delay_max = 0.3

[instagram]
# Instagram-specific configuration
default_min_delay = 5.0
default_max_delay = 30.0
retry_multiplier = 5
retry_min_wait = 5
retry_max_wait = 60
login_delay_min = 1.0
login_delay_max = 2.0
request_delay_min = 2.0
request_delay_max = 4.0
general_delay_min = 1.0
general_delay_max = 2.0
